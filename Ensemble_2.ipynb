{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12039538,"sourceType":"datasetVersion","datasetId":7575961}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgb\nfrom scipy.optimize import minimize\nfrom scipy.stats import randint as sp_randint\nimport time\nimport joblib\n\n# Load your training dataset\ntrain_df = pd.read_csv('/kaggle/input/solar-panels-performance/dataset/cleaned_data.csv')\n\n# Define features and target variable\nX = train_df.drop('efficiency', axis=1)\ny = train_df['efficiency']\n\n# Impute missing values\nimputer = SimpleImputer(strategy='mean')\nX_imputed = imputer.fit_transform(X)\nX = pd.DataFrame(X_imputed, columns=X.columns)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Target range: {y.min():.3f} to {y.max():.3f}\")\nprint(f\"Target mean: {y.mean():.3f}, std: {y.std():.3f}\")\n\n# Feature scaling\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Define base models with Randomized Search for hyperparameter tuning\ndef get_base_models():\n    # Define parameter distributions for Randomized Search\n    param_dist_xgb = {\n        'n_estimators': sp_randint(100, 300),\n        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n        'max_depth': sp_randint(3, 10),\n        'subsample': [0.6, 0.7, 0.8, 0.9],\n        'colsample_bytree': [0.6, 0.7, 0.8, 0.9]\n    }\n\n    param_dist_rf = {\n        'n_estimators': sp_randint(100, 300),\n        'max_depth': sp_randint(5, 20),\n        'min_samples_split': sp_randint(2, 11),\n        'min_samples_leaf': sp_randint(1, 11)\n    }\n\n    param_dist_mlp = {\n        'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n        'activation': ['relu', 'tanh'],\n        'solver': ['adam', 'sgd'],\n        'alpha': [0.0001, 0.001, 0.01]\n    }\n\n    # Initialize models\n    xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist', n_jobs=-1)\n    rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)\n    mlp_model = MLPRegressor(random_state=42, max_iter=1000)\n\n    # Perform Randomized Search for XGBRegressor\n    random_search_xgb = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist_xgb,\n                                            n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n    random_search_xgb.fit(X, y)\n\n    # Perform Randomized Search for RandomForestRegressor\n    random_search_rf = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist_rf,\n                                          n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n    random_search_rf.fit(X, y)\n\n    # Perform Randomized Search for MLPRegressor\n    random_search_mlp = RandomizedSearchCV(estimator=mlp_model, param_distributions=param_dist_mlp,\n                                           n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n    random_search_mlp.fit(X_scaled, y)\n\n    return [\n        random_search_xgb.best_estimator_,\n        random_search_rf.best_estimator_,\n        RandomForestRegressor(n_estimators=80, max_depth=15, random_state=123, n_jobs=-1),\n        SVR(kernel='rbf', C=1.0, epsilon=0.1),\n        CatBoostRegressor(iterations=200, learning_rate=0.05, depth=8, l2_leaf_reg=10, border_count=100,\n                          bootstrap_type='Bernoulli', random_strength=5, verbose=False, random_state=42, thread_count=-1),\n        lgb.LGBMRegressor(verbose=-1, n_estimators=300, max_bin=10000, random_state=42, n_jobs=-1),\n        random_search_mlp.best_estimator_\n    ]\n\n# Custom scoring function\ndef custom_score(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    return 100 * (1 - np.sqrt(mse))\n\n# Hill climbing for optimal weights\ndef hill_climbing_weights(oof_preds, y_true, n_iter=1000):\n    n_models = oof_preds.shape[1]\n\n    def objective(weights):\n        weights = np.array(weights)\n        weights = np.abs(weights)\n        weights = weights / np.sum(weights)\n        pred = np.dot(oof_preds, weights)\n        return -custom_score(y_true, pred)\n\n    best_weights = np.ones(n_models) / n_models\n    best_score = -objective(best_weights)\n\n    print(f\"Initial score: {best_score:.4f}\")\n\n    for i in range(n_iter):\n        perturbation = np.random.normal(0, 0.1, n_models)\n        new_weights = best_weights + perturbation\n        new_weights = np.abs(new_weights)\n        new_weights = new_weights / np.sum(new_weights)\n\n        new_score = -objective(new_weights)\n\n        if new_score > best_score:\n            best_weights = new_weights\n            best_score = new_score\n            if i % 100 == 0:\n                print(f\"Iteration {i}: New best score: {best_score:.4f}\")\n\n    return best_weights, best_score\n\nclass OptimizedStackingRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, n_folds=5, random_seeds=[42, 123, 456]):\n        self.n_folds = n_folds\n        self.random_seeds = random_seeds\n        self.models_dict = {}\n        self.weights = None\n        self.scaler = StandardScaler()\n        self.imputer = SimpleImputer(strategy='mean')\n\n    def fit(self, X, y):\n        print(\"=== Step 1: Generating Out-of-Fold Predictions ===\")\n        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n\n        X_imputed = self.imputer.fit_transform(X)\n        X_scaled = self.scaler.fit_transform(X_imputed)\n\n        base_models = get_base_models()\n        n_models = len(base_models)\n\n        oof_preds = np.zeros((X.shape[0], n_models))\n\n        for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n            print(f\"Processing fold {fold + 1}/{self.n_folds}\")\n\n            for i, model in enumerate(base_models):\n                if i in [2, 6]:  # Indices for models that need scaled data\n                    X_train_fold = X_scaled[train_idx]\n                    X_val_fold = X_scaled[val_idx]\n                else:\n                    X_train_fold = X_imputed[train_idx]\n                    X_val_fold = X_imputed[val_idx]\n\n                y_train_fold = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n\n                model.fit(X_train_fold, y_train_fold)\n                oof_preds[val_idx, i] = model.predict(X_val_fold)\n\n        print(\"\\n=== Individual Model OOF Scores ===\")\n        for i in range(n_models):\n            score = custom_score(y, oof_preds[:, i])\n            print(f\"Model {i+1} ({type(base_models[i]).__name__}): {score:.4f}\")\n\n        print(\"\\n=== Step 2: Optimizing Weights with Hill Climbing ===\")\n        self.weights, best_score = hill_climbing_weights(oof_preds, y)\n\n        print(f\"\\nOptimal weights: {self.weights}\")\n        print(f\"Best OOF score: {best_score:.4f}\")\n\n        print(\"\\n=== Step 3: Training Final Models on 100% Data ===\")\n        for seed_idx, seed in enumerate(self.random_seeds):\n            print(f\"Training with seed {seed} ({seed_idx + 1}/{len(self.random_seeds)})\")\n\n            models_for_seed = []\n            for i, base_model in enumerate(base_models):\n                model_params = base_model.get_params().copy()\n\n                if hasattr(base_model, 'random_state') and not isinstance(base_model, SVR):\n                    model_params['random_state'] = seed\n\n                if hasattr(base_model, 'n_estimators'):\n                    original_estimators = model_params.get('n_estimators', 100)\n                    model_params['n_estimators'] = int(original_estimators * 1.25)\n                elif hasattr(base_model, 'iterations'):\n                    original_iterations = model_params.get('iterations', 100)\n                    model_params['iterations'] = int(original_iterations * 1.25)\n\n                final_model = type(base_model)(**model_params)\n\n                if i in [2, 6]:  # Indices for models that need scaled data\n                    final_model.fit(X_scaled, y)\n                else:\n                    final_model.fit(X_imputed, y)\n\n                models_for_seed.append(final_model)\n\n            self.models_dict[seed] = models_for_seed\n\n        return self\n\n    def predict(self, X):\n        X_imputed = self.imputer.transform(X)\n        X_scaled = self.scaler.transform(X_imputed)\n\n        all_seed_preds = []\n\n        for seed in self.random_seeds:\n            models = self.models_dict[seed]\n            seed_preds = np.zeros((X.shape[0], len(models)))\n\n            for i, model in enumerate(models):\n                if i in [2, 6]:  # Indices for models that need scaled data\n                    seed_preds[:, i] = model.predict(X_scaled)\n                else:\n                    seed_preds[:, i] = model.predict(X_imputed)\n\n            weighted_pred = np.dot(seed_preds, self.weights)\n            all_seed_preds.append(weighted_pred)\n\n        final_pred = np.mean(all_seed_preds, axis=0)\n        return final_pred\n\n# Train the optimized model\nprint(\"=== Training CPU-Optimized Stacking Ensemble ===\")\nstart_time = time.time()\n\nstacking_model = OptimizedStackingRegressor(n_folds=5, random_seeds=[42, 123, 456])\nstacking_model.fit(X, y)\n\ntraining_time = time.time() - start_time\nprint(f\"\\nTotal training time: {training_time/60:.2f} minutes\")\n\n# Save the model\nmodel_path = '/kaggle/working/cpu_optimized_ensemble_model.pkl'\njoblib.dump(stacking_model, model_path)\nprint(f\"Model saved as '{model_path}'\")\n\n# Load test data and make predictions\nprint(\"\\n=== Making Test Predictions ===\")\ntest_data_path = '/kaggle/input/solar-panels-performance/dataset/test_data_processed.csv'\ntest_df = pd.read_csv(test_data_path)\n\nfeature_columns = X.columns\ntest_df_aligned = test_df[feature_columns]\n\npredictions = stacking_model.predict(test_df_aligned)\n\n# Create submission\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'efficiency': predictions\n})\n\nsubmission_path = '/kaggle/working/submission_cpu_optimized.csv'\nsubmission_df.to_csv(submission_path, index=False)\n\nprint(f\"Submission saved as {submission_path}\")\nprint(f\"Prediction range: {predictions.min():.3f} to {predictions.max():.3f}\")\nprint(f\"Prediction mean: {predictions.mean():.3f}\")\nprint(f\"Final model uses {len(stacking_model.random_seeds)} different seeds with hill-climbing optimized weights\")\n\n# Optional: Quick validation check\nprint(\"\\n=== Quick Validation Check ===\")\nscorer = make_scorer(custom_score, greater_is_better=True)\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\ncv_scores = cross_val_score(stacking_model, X, y, cv=kfold, scoring=scorer)\nprint(f\"5-Fold CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T11:54:16.775356Z","iopub.execute_input":"2025-06-03T11:54:16.775666Z","iopub.status.idle":"2025-06-03T12:24:07.802124Z","shell.execute_reply.started":"2025-06-03T11:54:16.775643Z","shell.execute_reply":"2025-06-03T12:24:07.801349Z"}},"outputs":[{"name":"stdout","text":"Dataset shape: (18513, 11)\nTarget range: 0.000 to 0.951\nTarget mean: 0.505, std: 0.138\n=== Training CPU-Optimized Stacking Ensemble ===\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.5176\nModel 2 (RandomForestRegressor): 89.5027\nModel 3 (RandomForestRegressor): 89.2553\nModel 4 (SVR): 88.9334\nModel 5 (CatBoostRegressor): 89.4515\nModel 6 (LGBMRegressor): 89.3689\nModel 7 (MLPRegressor): 88.9127\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.5187\n\nOptimal weights: [0.33603501 0.36354131 0.00294165 0.0048744  0.0648906  0.18021186\n 0.04750517]\nBest OOF score: 89.5653\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n\nTotal training time: 5.44 minutes\nModel saved as '/kaggle/working/cpu_optimized_ensemble_model.pkl'\n\n=== Making Test Predictions ===\nSubmission saved as /kaggle/working/submission_cpu_optimized.csv\nPrediction range: 0.247 to 0.885\nPrediction mean: 0.512\nFinal model uses 3 different seeds with hill-climbing optimized weights\n\n=== Quick Validation Check ===\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.5107\nModel 2 (RandomForestRegressor): 89.4823\nModel 3 (RandomForestRegressor): 89.2725\nModel 4 (SVR): 88.8539\nModel 5 (CatBoostRegressor): 89.4831\nModel 6 (LGBMRegressor): 89.3328\nModel 7 (MLPRegressor): 88.8894\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.5151\nIteration 0: New best score: 89.5274\n\nOptimal weights: [0.30269742 0.21308135 0.01333756 0.00390232 0.26113736 0.19102642\n 0.01481757]\nBest OOF score: 89.5639\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.4497\nModel 2 (RandomForestRegressor): 89.4058\nModel 3 (RandomForestRegressor): 89.1507\nModel 4 (SVR): 88.8684\nModel 5 (CatBoostRegressor): 89.4144\nModel 6 (LGBMRegressor): 89.2522\nModel 7 (MLPRegressor): 88.8568\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.4503\n\nOptimal weights: [0.32681639 0.14347349 0.00045673 0.00451181 0.36458066 0.13123784\n 0.02892308]\nBest OOF score: 89.4921\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.5105\nModel 2 (RandomForestRegressor): 89.5139\nModel 3 (RandomForestRegressor): 89.2813\nModel 4 (SVR): 88.9032\nModel 5 (CatBoostRegressor): 89.4909\nModel 6 (LGBMRegressor): 89.3583\nModel 7 (MLPRegressor): 88.8664\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.5311\n\nOptimal weights: [0.12315795 0.36342087 0.01449039 0.0131565  0.27515106 0.20986856\n 0.00075467]\nBest OOF score: 89.5805\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.5137\nModel 2 (RandomForestRegressor): 89.5074\nModel 3 (RandomForestRegressor): 89.2531\nModel 4 (SVR): 88.9002\nModel 5 (CatBoostRegressor): 89.4993\nModel 6 (LGBMRegressor): 89.3214\nModel 7 (MLPRegressor): 88.8613\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.5228\nIteration 0: New best score: 89.5251\n\nOptimal weights: [0.30371776 0.20231269 0.03427823 0.00125622 0.29071044 0.14028133\n 0.02744334]\nBest OOF score: 89.5709\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n=== Step 1: Generating Out-of-Fold Predictions ===\nProcessing fold 1/5\nProcessing fold 2/5\nProcessing fold 3/5\nProcessing fold 4/5\nProcessing fold 5/5\n\n=== Individual Model OOF Scores ===\nModel 1 (XGBRegressor): 89.4312\nModel 2 (RandomForestRegressor): 89.3909\nModel 3 (RandomForestRegressor): 89.1433\nModel 4 (SVR): 88.8047\nModel 5 (CatBoostRegressor): 89.3792\nModel 6 (LGBMRegressor): 89.2104\nModel 7 (MLPRegressor): 88.7781\n\n=== Step 2: Optimizing Weights with Hill Climbing ===\nInitial score: 89.4215\nIteration 0: New best score: 89.4290\n\nOptimal weights: [0.39111933 0.29010208 0.00962916 0.01119368 0.17067097 0.1211118\n 0.00617299]\nBest OOF score: 89.4715\n\n=== Step 3: Training Final Models on 100% Data ===\nTraining with seed 42 (1/3)\nTraining with seed 123 (2/3)\nTraining with seed 456 (3/3)\n5-Fold CV Score: 89.5649 (+/- 0.3733)\n","output_type":"stream"}],"execution_count":7}]}

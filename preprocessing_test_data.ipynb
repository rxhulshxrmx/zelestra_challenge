{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcuml\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor \u001b[38;5;28;01mas\u001b[39;00m cuRandomForestRegressor\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Ridge\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KFold, cross_val_score\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cuml.ensemble import RandomForestRegressor as cuRandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import joblib\n",
    "\n",
    "# Load your training dataset\n",
    "train_df = pd.read_csv('dataset/cleaned_data.csv')\n",
    "\n",
    "# Define features and target variable\n",
    "X = train_df.drop('efficiency', axis=1)\n",
    "y = train_df['efficiency']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target range: {y.min():.3f} to {y.max():.3f}\")\n",
    "print(f\"Target mean: {y.mean():.3f}, std: {y.std():.3f}\")\n",
    "\n",
    "# Feature scaling for models that need it\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define the ImprovedStackingRegressor class\n",
    "class ImprovedStackingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, base_models, meta_model, use_scaling=True):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.use_scaling = use_scaling\n",
    "        if use_scaling:\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.use_scaling:\n",
    "            X_processed = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_processed = X\n",
    "\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        base_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            fold_predictions = np.zeros(X.shape[0])\n",
    "\n",
    "            for train_idx, val_idx in kf.split(X):\n",
    "                if i < 1:  # Only the first model uses scaled data\n",
    "                    X_train_fold = X_processed[train_idx]\n",
    "                    X_val_fold = X_processed[val_idx]\n",
    "                else:\n",
    "                    X_train_fold = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
    "                    X_val_fold = X.iloc[val_idx] if hasattr(X, 'iloc') else X[val_idx]\n",
    "\n",
    "                y_train_fold = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
    "\n",
    "                model.fit(X_train_fold, y_train_fold)\n",
    "                fold_predictions[val_idx] = model.predict(X_val_fold)\n",
    "\n",
    "            base_predictions[:, i] = fold_predictions\n",
    "\n",
    "        self.meta_model.fit(base_predictions, y)\n",
    "\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            if i < 1:\n",
    "                model.fit(X_processed, y)\n",
    "            else:\n",
    "                model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.use_scaling:\n",
    "            X_processed = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_processed = X\n",
    "\n",
    "        base_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            if i < 1:\n",
    "                base_predictions[:, i] = model.predict(X_processed)\n",
    "            else:\n",
    "                base_predictions[:, i] = model.predict(X)\n",
    "\n",
    "        return self.meta_model.predict(base_predictions)\n",
    "\n",
    "# Define optimized base models with better hyperparameters\n",
    "base_models = [\n",
    "    XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method='hist',  # Use 'hist' tree method\n",
    "        device='cuda'  # Set device to CUDA for GPU training\n",
    "    ),\n",
    "    cuRandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_streams=1  # Set n_streams to 1 for reproducibility\n",
    "    ),\n",
    "    cuRandomForestRegressor(\n",
    "        n_estimators=150,\n",
    "        max_depth=20,\n",
    "        random_state=123,\n",
    "        n_streams=1  # Set n_streams to 1 for reproducibility\n",
    "    )\n",
    "]\n",
    "\n",
    "# Use Ridge regression as meta-model\n",
    "meta_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Create stacking model\n",
    "stacking_model = ImprovedStackingRegressor(\n",
    "    base_models=base_models,\n",
    "    meta_model=meta_model,\n",
    "    use_scaling=True\n",
    ")\n",
    "\n",
    "# Define the same scoring method as your original code\n",
    "def custom_score(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return 100 * (1 - np.sqrt(mse))\n",
    "\n",
    "scorer = make_scorer(custom_score, greater_is_better=True)\n",
    "\n",
    "# Set up k-fold cross-validation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Test individual models first\n",
    "print(\"\\n=== Individual Model Performance ===\")\n",
    "for i, model in enumerate(base_models):\n",
    "    if i < 1:\n",
    "        cv_scores = cross_val_score(model, X_scaled, y, cv=kfold, scoring=scorer)\n",
    "    else:\n",
    "        cv_scores = cross_val_score(model, X, y, cv=kfold, scoring=scorer)\n",
    "\n",
    "    print(f\"Model {i+1} CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std()*2:.3f})\")\n",
    "\n",
    "# Test the stacking ensemble\n",
    "print(\"\\n=== Stacking Ensemble Performance ===\")\n",
    "cv_scores = cross_val_score(stacking_model, X, y, cv=kfold, scoring=scorer)\n",
    "\n",
    "print(\"Cross-Validation Scores:\", [f\"{score:.3f}\" for score in cv_scores])\n",
    "print(f\"Mean CV Score: {cv_scores.mean():.3f}\")\n",
    "print(f\"Std CV Score: {cv_scores.std():.3f}\")\n",
    "\n",
    "# Train final model\n",
    "print(\"\\n=== Training Final Model ===\")\n",
    "stacking_model.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(stacking_model, 'optimized_ensemble_model.pkl')\n",
    "print(\"Model saved as 'optimized_ensemble_model.pkl'\")\n",
    "\n",
    "# Load the test data\n",
    "test_data_path = 'dataset/test.csv'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Preprocess the test data\n",
    "def preprocess_test_data(test_df, scaler=None):\n",
    "    # Convert columns to appropriate data types\n",
    "    test_df['humidity'] = pd.to_numeric(test_df['humidity'], errors='coerce')\n",
    "    test_df['wind_speed'] = pd.to_numeric(test_df['wind_speed'], errors='coerce')\n",
    "    test_df['pressure'] = pd.to_numeric(test_df['pressure'], errors='coerce')\n",
    "\n",
    "    # Fill missing panel_age with the median age of the same string_id\n",
    "    test_df['panel_age'] = test_df.groupby('string_id')['panel_age'].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "    # Impute missing values for maintenance_count\n",
    "    error_string_median = test_df.groupby(['string_id', 'error_code'])['maintenance_count'].median()\n",
    "    error_code_median = test_df.groupby('error_code')['maintenance_count'].median()\n",
    "    string_id_median = test_df.groupby('string_id')['maintenance_count'].median()\n",
    "    overall_median = test_df['maintenance_count'].median()\n",
    "\n",
    "    missing_mask = test_df['maintenance_count'].isna()\n",
    "    for idx in test_df[missing_mask].index:\n",
    "        string_id = test_df.loc[idx, 'string_id']\n",
    "        error_code = test_df.loc[idx, 'error_code']\n",
    "\n",
    "        if pd.notna(error_code) and (string_id, error_code) in error_string_median:\n",
    "            test_df.loc[idx, 'maintenance_count'] = error_string_median[(string_id, error_code)]\n",
    "        elif pd.notna(error_code) and error_code in error_code_median:\n",
    "            test_df.loc[idx, 'maintenance_count'] = error_code_median[error_code]\n",
    "        elif string_id in string_id_median:\n",
    "            test_df.loc[idx, 'maintenance_count'] = string_id_median[string_id]\n",
    "        else:\n",
    "            test_df.loc[idx, 'maintenance_count'] = overall_median\n",
    "\n",
    "    # Impute missing values for soiling_ratio\n",
    "    maintenance_bins = pd.cut(test_df['maintenance_count'], bins=5, include_lowest=True)\n",
    "    maintenance_soiling_median = test_df.groupby(maintenance_bins)['soiling_ratio'].median()\n",
    "\n",
    "    missing_soiling_mask = test_df['soiling_ratio'].isna()\n",
    "    for idx in test_df[missing_soiling_mask].index:\n",
    "        maintenance_val = test_df.loc[idx, 'maintenance_count']\n",
    "\n",
    "        if pd.notna(maintenance_val):\n",
    "            for bin_range, median_soiling in maintenance_soiling_median.items():\n",
    "                if maintenance_val >= bin_range.left and maintenance_val <= bin_range.right:\n",
    "                    test_df.loc[idx, 'soiling_ratio'] = median_soiling\n",
    "                    break\n",
    "\n",
    "    # Impute missing values for module_temperature\n",
    "    if 'temperature' in test_df.columns:\n",
    "        temp_numeric = pd.to_numeric(test_df['temperature'], errors='coerce')\n",
    "        module_temp_numeric = pd.to_numeric(test_df['module_temperature'], errors='coerce')\n",
    "\n",
    "        train_mask = module_temp_numeric.notna() & temp_numeric.notna()\n",
    "        if train_mask.sum() > 10:\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(temp_numeric[train_mask].values.reshape(-1, 1), module_temp_numeric[train_mask])\n",
    "\n",
    "            predict_mask = module_temp_numeric.isna() & temp_numeric.notna()\n",
    "            test_df.loc[predict_mask, 'module_temperature'] = lr.predict(temp_numeric[predict_mask].values.reshape(-1, 1))\n",
    "\n",
    "    numeric_features = test_df.select_dtypes(include=['number']).columns.tolist()\n",
    "    features_to_use = [f for f in numeric_features if f != 'module_temperature']\n",
    "\n",
    "    if features_to_use:\n",
    "        df_numeric = test_df[['module_temperature'] + features_to_use].apply(pd.to_numeric, errors='coerce')\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed_values = imputer.fit_transform(df_numeric)\n",
    "        test_df['module_temperature'] = imputed_values[:, 0]\n",
    "\n",
    "    # Create the 'power' feature\n",
    "    test_df['power'] = test_df['current'] * test_df['voltage']\n",
    "\n",
    "    # Impute missing values for irradiance\n",
    "    irradiance = pd.to_numeric(test_df['irradiance'], errors='coerce')\n",
    "    zero_mask = (irradiance == 0)\n",
    "    to_impute = zero_mask | irradiance.isna()\n",
    "\n",
    "    if 'power' in test_df.columns and 'efficiency' in test_df.columns:\n",
    "        estimated_irradiance = pd.to_numeric(test_df['power'], errors='coerce') / (pd.to_numeric(test_df['efficiency'], errors='coerce') + 0.0001)\n",
    "        plausible_mask = (estimated_irradiance > 0) & (estimated_irradiance < 1500)\n",
    "        irradiance[to_impute & plausible_mask] = estimated_irradiance[to_impute & plausible_mask]\n",
    "\n",
    "    relevant_features = ['temperature', 'module_temperature', 'humidity', 'cloud_coverage']\n",
    "    use_features = [f for f in relevant_features if f in test_df.columns]\n",
    "\n",
    "    if use_features:\n",
    "        knn_data = test_df[use_features].apply(pd.to_numeric, errors='coerce')\n",
    "        knn_data['irradiance'] = irradiance\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed = imputer.fit_transform(knn_data)\n",
    "        irradiance[to_impute] = imputed[to_impute, -1]\n",
    "\n",
    "    irradiance = np.clip(irradiance, 0, 1500)\n",
    "    if irradiance.isna().any():\n",
    "        irradiance.fillna(irradiance.median(), inplace=True)\n",
    "    test_df['irradiance'] = irradiance\n",
    "\n",
    "    # Impute missing values for humidity\n",
    "    humidity = pd.to_numeric(test_df['humidity'], errors='coerce')\n",
    "    zero_mask = (humidity == 0)\n",
    "    to_impute = zero_mask | humidity.isna()\n",
    "\n",
    "    relevant_features = ['temperature', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure']\n",
    "    use_features = [f for f in relevant_features if f in test_df.columns]\n",
    "\n",
    "    if use_features:\n",
    "        knn_data = test_df[use_features].apply(pd.to_numeric, errors='coerce')\n",
    "        knn_data['humidity'] = humidity\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed = imputer.fit_transform(knn_data)\n",
    "        humidity[to_impute] = imputed[to_impute, -1]\n",
    "\n",
    "    humidity = np.clip(humidity, 0, 100)\n",
    "    if humidity.isna().any():\n",
    "        humidity.fillna(humidity.median(), inplace=True)\n",
    "    test_df['humidity'] = humidity\n",
    "\n",
    "    # Handle categorical variables: one-hot encode 'error_code'\n",
    "    test_df = pd.get_dummies(test_df, columns=['error_code'])\n",
    "\n",
    "    # Ensure all one-hot encoded columns from training are present\n",
    "    expected_columns = ['error_code_E00', 'error_code_E01', 'error_code_E02', 'error_code_Unknown']\n",
    "    for column in expected_columns:\n",
    "        if column not in test_df.columns:\n",
    "            test_df[column] = 0\n",
    "\n",
    "    # Scale numerical features using the same scaler as the training data\n",
    "    if scaler:\n",
    "        numerical_features = ['module_temperature', 'irradiance', 'power', 'panel_age', 'maintenance_count', 'soiling_ratio', 'humidity']\n",
    "        test_df[numerical_features] = scaler.transform(test_df[numerical_features])\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# Load the scaler used for training data\n",
    "scaler = joblib.load('scaler.pkl')  # Make sure to replace 'scaler.pkl' with the actual path to your saved scaler\n",
    "\n",
    "# Preprocess the test data\n",
    "test_df_processed = preprocess_test_data(test_df, scaler)\n",
    "\n",
    "# Ensure the test data has the same features as the training data\n",
    "training_feature_columns = X.columns.tolist()\n",
    "test_df_aligned = test_df_processed[['id'] + [col for col in training_feature_columns if col in test_df_processed.columns]]\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = stacking_model.predict(test_df_aligned[training_feature_columns])\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df_aligned['id'],\n",
    "    'efficiency': predictions\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved as {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

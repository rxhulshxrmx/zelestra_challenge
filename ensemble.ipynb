{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: xgboost in ./venv/lib/python3.12/site-packages (3.0.2)\n",
      "Collecting catboost\n",
      "  Using cached catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: lightgbm in ./venv/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting graphviz (from catboost)\n",
      "  Using cached graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from catboost) (3.10.3)\n",
      "Collecting plotly (from catboost)\n",
      "  Using cached plotly-6.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.3)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Using cached narwhals-1.41.0-py3-none-any.whl.metadata (11 kB)\n",
      "Using cached catboost-1.2.8-cp312-cp312-macosx_11_0_universal2.whl (27.8 MB)\n",
      "Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "Using cached plotly-6.1.2-py3-none-any.whl (16.3 MB)\n",
      "Using cached narwhals-1.41.0-py3-none-any.whl (357 kB)\n",
      "Installing collected packages: narwhals, graphviz, plotly, catboost\n",
      "Successfully installed catboost-1.2.8 graphviz-0.20.3 narwhals-1.41.0 plotly-6.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost catboost lightgbm joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Processed Data ===\n",
      "Dataset shape: (20000, 19)\n",
      "Columns: ['id', 'temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure', 'power', 'adjusted_irradiance', 'string_id_encoded', 'error_code_encoded', 'installation_type_encoded', 'efficiency']\n",
      "\n",
      "Features shape: (20000, 17)\n",
      "Features: ['temperature', 'irradiance', 'humidity', 'panel_age', 'maintenance_count', 'soiling_ratio', 'voltage', 'current', 'module_temperature', 'cloud_coverage', 'wind_speed', 'pressure', 'power', 'adjusted_irradiance', 'string_id_encoded', 'error_code_encoded', 'installation_type_encoded']\n",
      "Target range: 0.0000 to 0.9871\n",
      "\n",
      "Train set: (16000, 17)\n",
      "Validation set: (4000, 17)\n",
      "\n",
      "=== Defining Models ===\n",
      "\n",
      "=== Training Individual Models ===\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost validation score: 88.9981\n",
      "\n",
      "Training RandomForest...\n",
      "RandomForest validation score: 89.1203\n",
      "\n",
      "Training CatBoost...\n",
      "CatBoost validation score: 89.2913\n",
      "\n",
      "Training LightGBM...\n",
      "LightGBM validation score: 89.1254\n",
      "\n",
      "Training Ridge...\n",
      "Ridge validation score: 89.0262\n",
      "\n",
      "=== Creating Ensemble Model ===\n",
      "Ensemble validation score: 89.2690\n",
      "\n",
      "=== Cross-Validation on Full Dataset ===\n",
      "Fold 1/5...\n",
      "Fold 2/5...\n",
      "Fold 3/5...\n",
      "Fold 4/5...\n",
      "Fold 5/5...\n",
      "\n",
      "=== Final Results ===\n",
      "Best individual model: CatBoost\n",
      "Individual model CV score: 89.5534 (+/- 0.6616)\n",
      "Ensemble model CV score: 89.5506 (+/- 0.6854)\n",
      "\n",
      "=== Training Final Model on Full Dataset ===\n",
      "Using CatBoost as final model\n",
      "\n",
      "Final model saved as: solar_panel_catboost_model.pkl\n",
      "\n",
      "=== Feature Importance ===\n",
      "Top 10 most important features:\n",
      "                feature  importance\n",
      "13  adjusted_irradiance   40.657158\n",
      "3             panel_age   11.041650\n",
      "1            irradiance    9.282101\n",
      "12                power    5.930277\n",
      "2              humidity    4.777719\n",
      "5         soiling_ratio    4.046403\n",
      "7               current    3.817288\n",
      "11             pressure    3.198149\n",
      "9        cloud_coverage    3.054295\n",
      "0           temperature    2.842707\n",
      "\n",
      "=== Summary ===\n",
      "✅ Used 17 features for training\n",
      "✅ Final model: CatBoost\n",
      "✅ Cross-validation score: 89.5534\n",
      "✅ Model saved as: solar_panel_catboost_model.pkl\n",
      "✅ Ready for test predictions!\n",
      "\n",
      "=== Quick Prediction Test ===\n",
      "Sample predictions: [0.54832516 0.40339773 0.62888803 0.59716495 0.37561946]\n",
      "Actual values: [0.56209625 0.39644739 0.57377604 0.62900923 0.34187372]\n",
      "Prediction range: 0.3756 to 0.6289\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load your processed data\n",
    "print(\"=== Loading Processed Data ===\")\n",
    "train_df = pd.read_csv('processed_dataset/train.csv')\n",
    "\n",
    "print(f\"Dataset shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "\n",
    "# Prepare features and target\n",
    "# Use ALL columns except 'id' and 'efficiency'\n",
    "feature_cols = [col for col in train_df.columns if col not in ['id', 'efficiency']]\n",
    "X = train_df[feature_cols]\n",
    "y = train_df['efficiency']\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Features: {feature_cols}\")\n",
    "print(f\"Target range: {y.min():.4f} to {y.max():.4f}\")\n",
    "\n",
    "# Custom scoring function (same as competition)\n",
    "def custom_score(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return 100 * (1 - np.sqrt(mse))\n",
    "\n",
    "# Split data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "\n",
    "# Define models\n",
    "print(\"\\n=== Defining Models ===\")\n",
    "models = {\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=12,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostRegressor(\n",
    "        iterations=300,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "        thread_count=-1\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        verbose=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Ridge': Ridge(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Train individual models and evaluate\n",
    "print(\"\\n=== Training Individual Models ===\")\n",
    "individual_scores = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Predict on validation set\n",
    "    val_pred = model.predict(X_val)\n",
    "    score = custom_score(y_val, val_pred)\n",
    "    individual_scores[name] = score\n",
    "    \n",
    "    print(f\"{name} validation score: {score:.4f}\")\n",
    "\n",
    "# Create ensemble (Voting Regressor)\n",
    "print(\"\\n=== Creating Ensemble Model ===\")\n",
    "ensemble_models = [\n",
    "    ('xgb', models['XGBoost']),\n",
    "    ('rf', models['RandomForest']),\n",
    "    ('cat', models['CatBoost']),\n",
    "    ('lgb', models['LightGBM']),\n",
    "    ('ridge', models['Ridge'])\n",
    "]\n",
    "\n",
    "ensemble = VotingRegressor(ensemble_models)\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_pred = ensemble.predict(X_val)\n",
    "ensemble_score = custom_score(y_val, ensemble_pred)\n",
    "print(f\"Ensemble validation score: {ensemble_score:.4f}\")\n",
    "\n",
    "# Cross-validation on full dataset\n",
    "print(\"\\n=== Cross-Validation on Full Dataset ===\")\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# CV for best individual model\n",
    "best_model_name = max(individual_scores, key=individual_scores.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "cv_scores_individual = []\n",
    "cv_scores_ensemble = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "    print(f\"Fold {fold + 1}/5...\")\n",
    "    \n",
    "    X_train_fold = X.iloc[train_idx]\n",
    "    X_val_fold = X.iloc[val_idx]\n",
    "    y_train_fold = y.iloc[train_idx]\n",
    "    y_val_fold = y.iloc[val_idx]\n",
    "    \n",
    "    # Individual model\n",
    "    best_model.fit(X_train_fold, y_train_fold)\n",
    "    pred_individual = best_model.predict(X_val_fold)\n",
    "    score_individual = custom_score(y_val_fold, pred_individual)\n",
    "    cv_scores_individual.append(score_individual)\n",
    "    \n",
    "    # Ensemble model\n",
    "    ensemble.fit(X_train_fold, y_train_fold)\n",
    "    pred_ensemble = ensemble.predict(X_val_fold)\n",
    "    score_ensemble = custom_score(y_val_fold, pred_ensemble)\n",
    "    cv_scores_ensemble.append(score_ensemble)\n",
    "\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "print(f\"Best individual model: {best_model_name}\")\n",
    "print(f\"Individual model CV score: {np.mean(cv_scores_individual):.4f} (+/- {np.std(cv_scores_individual)*2:.4f})\")\n",
    "print(f\"Ensemble model CV score: {np.mean(cv_scores_ensemble):.4f} (+/- {np.std(cv_scores_ensemble)*2:.4f})\")\n",
    "\n",
    "# Train final model on full dataset\n",
    "print(f\"\\n=== Training Final Model on Full Dataset ===\")\n",
    "if np.mean(cv_scores_ensemble) > np.mean(cv_scores_individual):\n",
    "    final_model = ensemble\n",
    "    final_model_name = \"Ensemble\"\n",
    "    print(\"Using Ensemble as final model\")\n",
    "else:\n",
    "    final_model = best_model\n",
    "    final_model_name = best_model_name\n",
    "    print(f\"Using {best_model_name} as final model\")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Save the final model\n",
    "model_filename = f'solar_panel_{final_model_name.lower()}_model.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"\\nFinal model saved as: {model_filename}\")\n",
    "\n",
    "# Feature importance (if available)\n",
    "print(f\"\\n=== Feature Importance ===\")\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    importance = final_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    print(feature_importance.head(10))\n",
    "elif final_model_name == \"Ensemble\":\n",
    "    # For ensemble, get average importance from tree-based models\n",
    "    importances = []\n",
    "    for name, model in ensemble.named_estimators_.items():\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances.append(model.feature_importances_)\n",
    "    \n",
    "    if importances:\n",
    "        avg_importance = np.mean(importances, axis=0)\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': avg_importance\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 most important features (ensemble average):\")\n",
    "        print(feature_importance.head(10))\n",
    "\n",
    "print(f\"\\n=== Summary ===\")\n",
    "print(f\"✅ Used {len(feature_cols)} features for training\")\n",
    "print(f\"✅ Final model: {final_model_name}\")\n",
    "print(f\"✅ Cross-validation score: {np.mean(cv_scores_ensemble if final_model_name == 'Ensemble' else cv_scores_individual):.4f}\")\n",
    "print(f\"✅ Model saved as: {model_filename}\")\n",
    "print(f\"✅ Ready for test predictions!\")\n",
    "\n",
    "# Quick prediction test\n",
    "print(f\"\\n=== Quick Prediction Test ===\")\n",
    "sample_pred = final_model.predict(X.head(5))\n",
    "print(f\"Sample predictions: {sample_pred}\")\n",
    "print(f\"Actual values: {y.head(5).values}\")\n",
    "print(f\"Prediction range: {sample_pred.min():.4f} to {sample_pred.max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
